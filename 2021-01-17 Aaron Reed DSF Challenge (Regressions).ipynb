{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the necessities\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import warnings\n",
    "from scipy.stats import bartlett, levene\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sqlalchemy import create_engine\n",
    "from statsmodels.tools.eval_measures import mse, rmse\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the authorization variables\n",
    "postgres_user = 'dsbc_student'\n",
    "postgres_pw = '7*.8G9QH21'\n",
    "postgres_host = '142.93.121.174'\n",
    "postgres_port = '5432'\n",
    "houses_db = 'houseprices'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create and dispose of engine\n",
    "engine = create_engine('postgresql://{}:{}@{}:{}/{}'.format(\n",
    "    postgres_user, postgres_pw, postgres_host, postgres_port, houses_db))\n",
    "\n",
    "houseprices_df = pd.read_sql_query('SELECT * FROM houseprices', con=engine)\n",
    "\n",
    "engine.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we'll define a function here that will allow us to test quickly\n",
    "#below to tune to the best parameters and test after\n",
    "def tuning():\n",
    "    y = info_df['saleprice']\n",
    "    X = info_df.drop(['saleprice'], axis=1)\n",
    "    X = sm.add_constant(X)\n",
    "    results = sm.OLS(y, X).fit()\n",
    "    print(results.summary())\n",
    "\n",
    "def lrm_test():\n",
    "    y = info_df['saleprice']\n",
    "    X = info_df.drop(['saleprice'], axis=1)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=357)\n",
    "    X_train = sm.add_constant(X_train)\n",
    "    X_test = sm.add_constant(X_test)\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train, y_train)\n",
    "    y_train_predictions = lr.predict(X_train)\n",
    "    y_test_predictions = lr.predict(X_test)\n",
    "    print('OLS Regression')\n",
    "    print(\"R-squared of the model in the training set is: {}\".format(lr.score(X_train, y_train)))\n",
    "    print(\"R-squared of the model in the test set is: {}\".format(lr.score(X_test, y_test)))\n",
    "    print(\"\\nMean absolute error of the prediction is: {}\".format(mean_absolute_error(y_test, y_test_predictions)))\n",
    "    print(\"Mean squared error of the prediction is: {:3e}\".format(mse(y_test, y_test_predictions)))\n",
    "    print(\"Root mean squared error of the prediction is: {}\".format(rmse(y_test, y_test_predictions)))\n",
    "    print(\"Mean absolute percentage error of the prediction is: {}\".format(np.mean(np.abs((y_test - y_test_predictions) / y_test)) * 100))\n",
    "    \n",
    "def lasso_test():\n",
    "    y = info_df['saleprice']\n",
    "    X = info_df.drop(['saleprice'], axis=1)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=357)\n",
    "    X_train = sm.add_constant(X_train)\n",
    "    X_test = sm.add_constant(X_test)\n",
    "    lr = LassoCV(cv=10)\n",
    "    lr.fit(X_train, y_train)\n",
    "    y_train_predictions = lr.predict(X_train)\n",
    "    y_test_predictions = lr.predict(X_test)\n",
    "    print('Lasso Regression')\n",
    "    print(\"R-squared of the model in the training set is: {}\".format(lr.score(X_train, y_train)))\n",
    "    print(\"R-squared of the model in the test set is: {}\".format(lr.score(X_test, y_test)))\n",
    "    print(\"\\nMean absolute error of the prediction is: {}\".format(mean_absolute_error(y_test, y_test_predictions)))\n",
    "    print(\"Mean squared error of the prediction is: {:3e}\".format(mse(y_test, y_test_predictions)))\n",
    "    print(\"Root mean squared error of the prediction is: {}\".format(rmse(y_test, y_test_predictions)))\n",
    "    print(\"Mean absolute percentage error of the prediction is: {}\".format(np.mean(np.abs((y_test - y_test_predictions) / y_test)) * 100))\n",
    "\n",
    "def ridge_test():\n",
    "    y = info_df['saleprice']\n",
    "    X = info_df.drop(['saleprice'], axis=1)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=357)\n",
    "    X_train = sm.add_constant(X_train)\n",
    "    X_test = sm.add_constant(X_test)\n",
    "    lr = RidgeCV(cv=10)\n",
    "    lr.fit(X_train, y_train)\n",
    "    y_train_predictions = lr.predict(X_train)\n",
    "    y_test_predictions = lr.predict(X_test)\n",
    "    print('Ridge Regression')\n",
    "    print(\"R-squared of the model in the training set is: {}\".format(lr.score(X_train, y_train)))\n",
    "    print(\"R-squared of the model in the test set is: {}\".format(lr.score(X_test, y_test)))\n",
    "    print(\"\\nMean absolute error of the prediction is: {}\".format(mean_absolute_error(y_test, y_test_predictions)))\n",
    "    print(\"Mean squared error of the prediction is: {:3e}\".format(mse(y_test, y_test_predictions)))\n",
    "    print(\"Root mean squared error of the prediction is: {}\".format(rmse(y_test, y_test_predictions)))\n",
    "    print(\"Mean absolute percentage error of the prediction is: {}\".format(np.mean(np.abs((y_test - y_test_predictions) / y_test)) * 100))\n",
    "    \n",
    "def en_test():\n",
    "    y = info_df['saleprice']\n",
    "    X = info_df.drop(['saleprice'], axis=1)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=357)\n",
    "    X_train = sm.add_constant(X_train)\n",
    "    X_test = sm.add_constant(X_test)\n",
    "    lr = ElasticNetCV(cv=10)\n",
    "    lr.fit(X_train, y_train)\n",
    "    y_train_predictions = lr.predict(X_train)\n",
    "    y_test_predictions = lr.predict(X_test)\n",
    "    print('Elastic Net Regression')\n",
    "    print(\"R-squared of the model in the training set is: {}\".format(lr.score(X_train, y_train)))\n",
    "    print(\"R-squared of the model in the test set is: {}\".format(lr.score(X_test, y_test)))\n",
    "    print(\"\\nMean absolute error of the prediction is: {}\".format(mean_absolute_error(y_test, y_test_predictions)))\n",
    "    print(\"Mean squared error of the prediction is: {:3e}\".format(mse(y_test, y_test_predictions)))\n",
    "    print(\"Root mean squared error of the prediction is: {}\".format(rmse(y_test, y_test_predictions)))\n",
    "    print(\"Mean absolute percentage error of the prediction is: {}\".format(np.mean(np.abs((y_test - y_test_predictions) / y_test)) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tuning can be found at [https://github.com/CasualChemist/dsf_data/blob/main/2021-01-17%20Aaron%20Reed%20DSF%20Challenge%20(Regressions)%20Tests.ipynb]. For sake of space and to not rewrite the code, I will call the final model directly.\n",
    "Once it has been called, I'll run the tuning test and show the results. Afterwards, I will run the various types of regressions with a CV of 10 (when applicable) and print their stats for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_df = pd.read_csv('D:\\DSF\\houseprices_model.csv', sep='\\t', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:              saleprice   R-squared:                       0.900\n",
      "Model:                            OLS   Adj. R-squared:                  0.896\n",
      "Method:                 Least Squares   F-statistic:                     214.0\n",
      "Date:                Wed, 20 Jan 2021   Prob (F-statistic):               0.00\n",
      "Time:                        17:51:05   Log-Likelihood:                -16862.\n",
      "No. Observations:                1460   AIC:                         3.384e+04\n",
      "Df Residuals:                    1400   BIC:                         3.416e+04\n",
      "Df Model:                          59                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==========================================================================================\n",
      "                             coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------------------\n",
      "const                  -1.248e+06   1.15e+05    -10.840      0.000   -1.47e+06   -1.02e+06\n",
      "lotarea                    0.4468      0.083      5.404      0.000       0.285       0.609\n",
      "overallqual             8258.6626    968.411      8.528      0.000    6358.970    1.02e+04\n",
      "overallcond             6367.8068    736.159      8.650      0.000    4923.714    7811.900\n",
      "yearbuilt                338.9730     54.168      6.258      0.000     232.714     445.232\n",
      "masvnrarea                11.8084      4.688      2.519      0.012       2.612      21.005\n",
      "bsmtfinsf1                20.3999      1.924     10.603      0.000      16.626      24.174\n",
      "grlivarea                 27.4733      4.325      6.352      0.000      18.988      35.958\n",
      "bedroomabvgr           -4593.5717   1321.448     -3.476      0.001   -7185.803   -2001.341\n",
      "kitchenabvgr           -1.696e+04   3681.419     -4.608      0.000   -2.42e+04   -9742.798\n",
      "totrmsabvgrd            1989.0259    953.253      2.087      0.037     119.067    3858.985\n",
      "fireplaces              4663.3738   1337.218      3.487      0.001    2040.207    7286.540\n",
      "garagecars              7298.5340   1303.035      5.601      0.000    4742.423    9854.645\n",
      "wooddecksf                12.9691      5.999      2.162      0.031       1.201      24.738\n",
      "poolarea                  84.4915     18.435      4.583      0.000      48.328     120.655\n",
      "120                    -2.516e+04   3449.149     -7.294      0.000   -3.19e+04   -1.84e+04\n",
      "160                    -3.323e+04   4128.210     -8.051      0.000   -4.13e+04   -2.51e+04\n",
      "interiorsf                18.4511      2.508      7.356      0.000      13.531      23.371\n",
      "porchsf                   19.1071      7.187      2.658      0.008       5.008      33.206\n",
      "3 bath                  2.992e+04   5102.985      5.862      0.000    1.99e+04    3.99e+04\n",
      "on paved road           2.888e+04   1.14e+04      2.537      0.011    6554.012    5.12e+04\n",
      "Banked contour         -9907.1562   3591.442     -2.759      0.006    -1.7e+04   -2861.968\n",
      "Hillside contour        1.135e+04   3933.769      2.885      0.004    3632.273    1.91e+04\n",
      "on culdsac              8485.4162   2919.711      2.906      0.004    2757.936    1.42e+04\n",
      "BrkSide                -1.589e+04   4587.669     -3.463      0.001   -2.49e+04   -6886.978\n",
      "ClearCr                -1.913e+04   5926.700     -3.228      0.001   -3.08e+04   -7505.427\n",
      "CollgCr                -1.462e+04   3219.536     -4.539      0.000   -2.09e+04   -8299.382\n",
      "Edwards                -2.751e+04   3754.789     -7.327      0.000   -3.49e+04   -2.01e+04\n",
      "Gilbert                -2.078e+04   3953.965     -5.254      0.000   -2.85e+04    -1.3e+04\n",
      "IDOTRR                 -2.506e+04   5459.741     -4.589      0.000   -3.58e+04   -1.43e+04\n",
      "MeadowV                -2.021e+04   6894.452     -2.931      0.003   -3.37e+04   -6683.457\n",
      "Mitchel                 -2.68e+04   4556.385     -5.882      0.000   -3.57e+04   -1.79e+04\n",
      "NAmes                   -2.34e+04   3201.712     -7.307      0.000   -2.97e+04   -1.71e+04\n",
      "NWAmes                 -3.077e+04   4035.561     -7.626      0.000   -3.87e+04   -2.29e+04\n",
      "NoRidge                 2.537e+04   5137.042      4.938      0.000    1.53e+04    3.54e+04\n",
      "NridgHt                 1.939e+04   4044.447      4.793      0.000    1.15e+04    2.73e+04\n",
      "OldTown                -2.805e+04   4077.155     -6.879      0.000    -3.6e+04      -2e+04\n",
      "SWISU                  -2.142e+04   6258.101     -3.423      0.001   -3.37e+04   -9144.559\n",
      "Sawyer                 -2.406e+04   4088.422     -5.886      0.000   -3.21e+04    -1.6e+04\n",
      "SawyerW                -1.664e+04   4136.962     -4.022      0.000   -2.48e+04   -8523.065\n",
      "StoneBr                 3.219e+04   5798.178      5.552      0.000    2.08e+04    4.36e+04\n",
      "Timber                 -1.226e+04   5003.659     -2.451      0.014   -2.21e+04   -2448.018\n",
      "CompShg                 5.951e+05   3.11e+04     19.121      0.000    5.34e+05    6.56e+05\n",
      "Membran                 6.369e+05   4.11e+04     15.496      0.000    5.56e+05    7.18e+05\n",
      "Metal                   6.036e+05   4.14e+04     14.597      0.000    5.23e+05    6.85e+05\n",
      "Roll                    5.867e+05   4.06e+04     14.440      0.000    5.07e+05    6.66e+05\n",
      "Tar&Grv                 5.943e+05   3.18e+04     18.689      0.000    5.32e+05    6.57e+05\n",
      "WdShake                 5.992e+05   3.32e+04     18.051      0.000    5.34e+05    6.64e+05\n",
      "WdShngl                 6.736e+05   3.25e+04     20.723      0.000     6.1e+05    7.37e+05\n",
      "Plywood                -5023.0535   2912.949     -1.724      0.085   -1.07e+04     691.161\n",
      "Ext Good Qual          -2.064e+04   4188.932     -4.927      0.000   -2.89e+04   -1.24e+04\n",
      "Ext Avg Qual           -2.145e+04   4397.312     -4.878      0.000   -3.01e+04   -1.28e+04\n",
      "Poured Cncrt fndtn      6694.6246   2299.446      2.911      0.004    2183.894    1.12e+04\n",
      "Slab fndtn              1.288e+04   6128.096      2.101      0.036     854.270    2.49e+04\n",
      "Kitchen Fair Qual      -2.897e+04   5940.839     -4.876      0.000   -4.06e+04   -1.73e+04\n",
      "Kitchen Good Qual      -2.826e+04   3500.517     -8.073      0.000   -3.51e+04   -2.14e+04\n",
      "Kitchen Avg Qual       -2.985e+04   3961.195     -7.536      0.000   -3.76e+04   -2.21e+04\n",
      "Severe Deduction Fnctn -6.299e+04   2.73e+04     -2.310      0.021   -1.16e+05   -9488.545\n",
      "Normal Sale             6931.1301   2408.666      2.878      0.004    2206.146    1.17e+04\n",
      "Partial Sale            1.922e+04   3652.661      5.261      0.000    1.21e+04    2.64e+04\n",
      "==============================================================================\n",
      "Omnibus:                      791.362   Durbin-Watson:                   1.911\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           103939.289\n",
      "Skew:                          -1.510   Prob(JB):                         0.00\n",
      "Kurtosis:                      44.225   Cond. No.                     2.67e+06\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 2.67e+06. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "tuning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLS Regression\n",
      "R-squared of the model in the training set is: 0.9175940399135784\n",
      "R-squared of the model in the test set is: 0.7845623690347873\n",
      "\n",
      "Mean absolute error of the prediction is: 15827.69367927762\n",
      "Mean squared error of the prediction is: 1.071772e+09\n",
      "Root mean squared error of the prediction is: 32737.93509742331\n",
      "Mean absolute percentage error of the prediction is: 9.687289969495124\n"
     ]
    }
   ],
   "source": [
    "#calling the function for the OLS Regression\n",
    "lrm_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression\n",
      "R-squared of the model in the training set is: 0.8857570918687354\n",
      "R-squared of the model in the test set is: 0.8240016647831889\n",
      "\n",
      "Mean absolute error of the prediction is: 15656.79613136504\n",
      "Mean squared error of the prediction is: 8.755674e+08\n",
      "Root mean squared error of the prediction is: 29589.987445924755\n",
      "Mean absolute percentage error of the prediction is: 9.467783700230987\n"
     ]
    }
   ],
   "source": [
    "#calling the function for the Ridge Regression\n",
    "ridge_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Regression\n",
      "R-squared of the model in the training set is: 0.7176574297095706\n",
      "R-squared of the model in the test set is: 0.6457417755828971\n",
      "\n",
      "Mean absolute error of the prediction is: 24925.29258437263\n",
      "Mean squared error of the prediction is: 1.762386e+09\n",
      "Root mean squared error of the prediction is: 41980.77693584046\n",
      "Mean absolute percentage error of the prediction is: 15.019875404098535\n"
     ]
    }
   ],
   "source": [
    "#calling the function for the Lasso Regression\n",
    "lasso_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elastic Net Regression\n",
      "R-squared of the model in the training set is: 0.6168978264738623\n",
      "R-squared of the model in the test set is: 0.561674909142513\n",
      "\n",
      "Mean absolute error of the prediction is: 30944.21519302323\n",
      "Mean squared error of the prediction is: 2.180607e+09\n",
      "Root mean squared error of the prediction is: 46696.96624660817\n",
      "Mean absolute percentage error of the prediction is: 19.48570360643981\n"
     ]
    }
   ],
   "source": [
    "#calling the function for the Elastic Net Regression\n",
    "en_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results of the test, I would use Ridge Regression. While OLS has a higher R-squared value for the training set, the difference between training and testing set for Ridge is lower. Also, all of the secondary stats for Ridge are more preferable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the final part of the testing page, I added in mortgage rates to see if it would have an impact on the model; it did not. It's possible that if I added it before doing the tuning, it could have been made significant. Another thing to note is that the data comes from a different source and therefore its validity could be called into question."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
